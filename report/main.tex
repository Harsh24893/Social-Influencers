\documentclass[conference]{IEEEtran}
\usepackage{blindtext, graphicx}
% correct bad hyphenation here
\usepackage[english]{babel} % English language/hyphenation
\usepackage{array}
\usepackage{multirow}
\usepackage{amsmath}
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}
\usepackage{caption}
\usepackage{listings}
\usepackage{color}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}
\begin{document}

% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{A Comparative study of different Machine Learning techniques on Kaggle Twitter Social Influencers Data Set}
\author{\IEEEauthorblockN{Arora, Pragya\IEEEauthorrefmark{1},
Ghai, Piyush\IEEEauthorrefmark{2} and
Ramkrishnan, Navnith\IEEEauthorrefmark{4}}
\IEEEauthorblockA{Department of Computer Science \& Engineering,
The Ohio State University\\
Columbus, OH 43202\\
Email: \IEEEauthorrefmark{1}arora.170@osu.edu,
\IEEEauthorrefmark{2}ghai.8@osu.edu,
\IEEEauthorrefmark{3}ramkrishnan.1@osu.edu}}\maketitle


\begin{abstract}
%\boldmath
Abstract—The advent of social networks has made it significant to analyze social media data. These graph networks have insights on different trends which are interesting to study. Certain nodes act as sinks whereas others as sources emancipating various other nodes. Understanding these patterns will help in various applications related to National Security and even in experiments like Network A/B Testing where we new features are released to a treatment group restricting the control group. In our project we intended to experiment and analyze on one such pattern where we try to identify the influential node among a given pair. To understand such a behavior, we used a Twitter data set which had 11 features specific to each user. We applied machine learning techniques to understanding the underlying features and classify the task at hand. 
 
\end{abstract}
% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the journal you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals frown on math
% in the abstract anyway.

% Note that keywords are not normally used for peerreview papers.
\section{Introduction}
The growing importance of Social Networks has given an impetus to understand in detail about its implications because these networks influence millions of people. Contrary to print media or television, social media has a greater reach to audiences across the world and information exchange has never seen such a revolution. The need of the hour is to have the ability to understand the nuances of these networks because with more connectivity it becomes crucial to understand the behavior of different nodes.

Some nodes/users are more influential than the others with their behavior impacting a larger set of nodes in the graph. Literature in psychology and cognitive science suggest that ones’ life is greatly impacted by immediate surroundings and their influence. As social networks have become a part of our lives, it is worthwhile to identify such influential users. One application of such a study is to predict the general mood of the crowd during elections by analyzing the social network handles of the candidates.

In this project, we solve a Kaggle\cite{kaggle} challenge, in which influencers on Twitter are predicted from a set of features derived from user activity on the network, such as follower count, retweets received, etc. Rather than identifying influencers in the larger network, the goal is to identify given a pair of users, A and B, who is more influential. This is a binary-classification problem. To find an optimal prediction method, we applied pre-processing techniques and feature transformations. We used four different machine learning algorithms: Logistic Regression, Support Vector Machines (SVM), Neural Networks and Gradient Boosting, to model the data. We also tried varying some of the decision boundary parameters for the SVM by using different kernels and neural network classifiers and adjusted the activation functions on the hidden layer. Our approach includes a few methods that we did not see in previous attempts at this problem, specifically a logarithmic feature transformation, the application of feature selection and gradient boosting. Outside of gradient boosting, the model configurations which produced the best results used linear-like decision boundaries (Logistic Regression).
 
\section{DATASET}\label{sec:page-layout}

This section expands on the data set which we used to try the different models on. 

\subsection{Original Dataset}\label{sec:formatting}
The data set which we used came from a Kaggle contest. The data set had the following files : 
\begin{itemize}
  \item train.csv
  \item test.csv
  \item sample\_predicitions.csv
\end{itemize}

The train.csv file contains data points where each data point is a pair wise features for two users along with a binary value which reflects which user is more influential. If the value is 0 the first user is more influential and vice-verse. Each user has 11 unique features which are extracted from their twitter profile and recorded in numeric form. The 11 features extracted includes:

\begin{itemize}
  \item Followers
  \item Following
  \item Listed
  \item Mentions received
  \item Re-tweets received
  \item Mentions sent
  \item Re-tweets sent
  \item Posts
  \item Network Feature 1
  \item Network Feature 2
  \item Network Feature 2
\end{itemize}

Similarly we have features for two users in each data item in the testing file but the testing file has no class label. There are 5500 training samples and 5953 testing samples in our data set. Given a test sample, our job is to predict which individual in this test sample is more influential.

\subsection{Kaggle Competition}
The Kaggle competition is hosted at : \cite{kaggle2}. The evaluation for this competition is based on area under the ROC curve. \cite{roc}.

\subsection{Data Preprocessing
}\label{sec:formatting-text}
Since the data is a numerical data, we use some pre-processing techniques on it. The dataset consists of 22 attributes which is essentially 11 attributes for each user in contention. Since we have to compare who is more influential in the network, we subtract related attributes for each user, to reduce the dimensionality to 11. The following is the transformation applied : 
\begin{align*}
a\textsubscript{j} = x\textsubscript{j} - x\textsubscript{j+11}, j = 1,2,3 .... , 11. 
\end{align*}
Since the attributes are of a different scale, we also apply a logarithmic transformation on the data. The following is the transformation applied :
\begin{lstlisting}[language=Python, caption = Log Transformation ]
def transform_features(x):
    return np.log(1 + x)
\end{lstlisting}
All of these transformations are applied to all the models used. In addition to these, we also apply Z-Score normalization on the dataset for using Gausian Na{\"i}ve Bayes model.
\section{SYSTEM MODELS
}\label{sec:fig-tables}
\subsection{Baseline}\label{sec:cap-num}
For baseline, we tried a dumb baseline model, where we classified an influencer based on the number of followers, i.e. if X has more followers than Y, then X is more influential. Using this dumb baseline, we get an accuracy of about \textbf{70.2\%}. This result shows
that the number of followers is a strong indicator
of the influencers. However, 70.2\% is not a very satisfying result and hence we use it only as a benchmark. We know further experiment with more models and a bit more pre-processing. This also suggested that the data can be linearly separable if we choose the right set of features.

\subsection{Logistic Regression}\label{sec:colour-illustrations}
For Logistic Regression, we implemented our own model using two different loss functions : \textit{Logistic Loss} \& \textit{Exponential Loss}. We also used \textit{scikit-learn}'s Logistic Loss for comparison to our own implementation of Logistic Regression. We used Stochastic Gradient Descent for implementing the LR algorithm.

\subsection{SVM}\label{sec:colour-illustrations}
We applied the technique of SVM's with a linear kernel as well as a radial basis kernel function for SVM. For SVM we use scikit learn packages implementation. \cite{svm}

\subsection{Gaussian Na{\"i}ve Bayes}\label{sec:colour-illustrations}
Logistic regression and SVM are discriminative learning algorithms. Gaussian Na{\"i}ve Bayes, is a generative model. The distribution of the original data is not Gaussian, so we tried Z-Score normalization in order to make it as a Gaussian distribution. In Z-Score, we do the following for every value : 
\begin{align*}
z = \frac{x - \mu}{\sigma}
\end{align*}
This converts the data into 0 mean and deviation 1. We then used Gaussian Na{\"i}ve Bayes from \textit{sklearn} package. \cite{gnb}

\subsection{Neural Network}\label{sec:colour-illustrations}
We wanted to explore and see how the dataset behaves in a non-linear environment. The idea was to compare the traditional techniques alongside state of the art machine learning models which have gained a lot of importance today. Hence we implemented a basic Neural Network model in Python to see how it behaves in this classification task. Our model was customizable, we could add or remove any number of hidden layers with any number of units in each of them. To our surprise the model worked well but didn't outperform the traditional model. The intuition behind this is that the data set and the features we have are linearly separable. 

\subsection{Boosting}
We also used scikit-learn's boosting method : specifically, XgBoost \cite{xgboost}. Boosting is an ensemble classifier which combines several weak classifiers to form a hypothesis. Boosting also has advantages over other classifiers in the fact that it is more robust to overfitting on the training data. 

\section{Tuning the Models}
\subsection{Feature Selection}
We did a correlation plot of all the 11 features with the class label. 
This is represented in Figure \ref{fs_fig}. From the figure we see that there are some features which will not be useful. We were thus motivated to further prune features using a feature selction algorithm.

\begin{figure}
\centering
  \includegraphics[width=200px, height = 200px]{scatter_plt}
  \caption{Scatter plot for co-relation among all the features }
  \label{fs_fig}
\end{figure}
We used forward selection algorithm to zero in on the best features in order to improve the test accuracy. We used Logistic Regression model in order to select the best accuracies for feature selection. The best features are as follows :
\begin{itemize}
\item Follower Count
\item Listed\_Count
\item Retweets Received
\item Network Feature 1
\item Mentions\_Received
\item Network\_Feature2
\end{itemize}

We note that \textit{network\_feature 2} was not showing up as important in the correlation plot, but it's addition to the best features set led to an improvement in accuracy.

\subsection{k- fold Cross Validation}
We used k-fold cross validation with k=20, where the data is partitioned into k subsets, with k-1 for training and the remaining for test. The results are averaged across all the iterations for uniformity.    

\section{Results \& Discussions}
\subsection{Evaluation Criteria}
The metric that we use to quantify our results was \textit{Area Under the ROC Curve}. ROC is a standard evaluation metric for binary classification problems. We used probabalistic outcomes from the models in order to plot the ROC Curves. \\
For comparing two models, or results from the same model, we compare their AUC values. A ROC Curve with a larger area signifies a better classification model on the given data.\\
To improve our evaluations, we applied k-fold cross validation technique. We try this with a k value of 20. In k-fold cross validation, the model is split into k subsets and is run for k iterations, where k-1 subsets are used to train. The AUC results are averaged across all the iterations.

\subsection{Logistic Regression (Our Implementation)}
For our implementation of Logistic Regression, we tried different loss functions - \textit{Logistic Loss}, \textit{Exponential Loss}. The hyperparameters tuned were : \textit{$\lambda$} (L2 Regularization) \& \textit{$\eta$} (Learning Rate). The results are summarized in Table \ref{tab_lr}. The results in this table are calculated at $\lambda = 0.1 \& \eta = 0.001$.

\begin{table}[!htb]
 \centering
 \caption{AUC for Logistic Regression}
 \label{tab_lr}
\begin{tabular}{ c c c c } 
	    \noalign{\smallskip}\hline\noalign{\smallskip}
		Data &  Loss Function & AUC \\
    	   \noalign{\smallskip}\hline\noalign{\smallskip}
		Train &  Logistic Loss & 0.850868\\
		Dev set & Logistic Loss & 0.8838471\\
		\noalign{\smallskip}\hline\noalign{\smallskip}
		Train &  Exponential Loss & 0.8572271\\
		Dev set & Exponential Loss & 0.88924958\\
		\noalign{\smallskip}\hline\noalign{\smallskip}	
  \end{tabular} 
\end{table}

\subsection{Logistic Regression (From scikit-learn)}
We also tried using Logistic Regression from \textit{scikit-learn} package. We tried tuning the \textit{C} parameter, which is inverse of 	regularization. The best value of C was found to be 100. The following table lists some results. We also evaluate the results with and without feature selection. 

\begin{table}[!htb]
 \centering
 \caption{AUC for Logistic Regression, C = 100}
 \label{tab_lr}
\begin{tabular}{ c c c c } 
	    \noalign{\smallskip}\hline\noalign{\smallskip}
		Data &  Feature Selection & AUC \\
    	   \noalign{\smallskip}\hline\noalign{\smallskip}
		Train &  Yes & 0.8574375\\
		Dev set & Yes & 0.89348\\
		\noalign{\smallskip}\hline\noalign{\smallskip}
		Train &  No & 0.849165911\\
		Dev set & No & 0.88155468\\
		\noalign{\smallskip}\hline\noalign{\smallskip}	
  \end{tabular} 
\end{table}

In figure \ref{lr_fig} we represent the AUC for Logistic Regression, where the area under the ROC curve is 0.89348.

\begin{figure}
\centering
  \includegraphics[width=200px, height = 200px]{lr_}
  \caption{AUC for Logistic Regression}
  \label{lr_fig}
\end{figure}

So we see, that we get nearly comparable results with both the Logistic Regression implementations, indicating that the data is linearly separable. We thus move onto more linear models.

\subsection{Support Vector Machine (RBF)}
We tried different regularization parameters. The only hyperparameter tuned was : \textit{$C$} (Slack Penalty). The results are summarized in Table \ref{tab_svm2}. The results in this table are calculated at $C = 0.3$.

\begin{table}[!htb]
 \centering
 \caption{AUC for SVM (rbf)}
 \label{tab_svm2}
\begin{tabular}{ c c c } 
	    \noalign{\smallskip}\hline\noalign{\smallskip}
		Data &   AUC \\
    	   \noalign{\smallskip}\hline\noalign{\smallskip}
		Train & 0.9288767\\
		Dev set & 0.8558008\\
				\noalign{\smallskip}\hline\noalign{\smallskip}	
  \end{tabular} 
\end{table}

\subsection{Support Vector Machine (Linear)}
We tried different regularization parameters. The only hyperparameter tuned was : \textit{$C$} (Slack Penalty). The results are summarized in Table \ref{tab_svm}.  The results in this table are calculated at $C = 0.3$.

\begin{table}[!htb]
 \centering
 \caption{AUC for SVM (linear)}
 \label{tab_svm}
\begin{tabular}{ c c c } 
	    \noalign{\smallskip}\hline\noalign{\smallskip}
		Data &   AUC \\
    	   \noalign{\smallskip}\hline\noalign{\smallskip}
		Train & 0.8596409\\
		Dev set & 0.8929928\\
			    \noalign{\smallskip}\hline\noalign{\smallskip}
  \end{tabular} 
\end{table}

So we see, that we get nearly different results with different SVM kernel implementations. This is because the data is linearly separable and hence the linear kernel performs so well.

\subsection{Gaussian Na{\"i}ve Bayes}
With Gaussian Na{\"i}ve Bayes we normalize the data using Z-score normalization, as mentioned in the Pre-Processing. The AUC is presented in Table \ref{nb_tab}.

\begin{table}[!htb]
 \centering
 \caption{AUC for Gaussian Na{\"i}ve Bayes}
 \label{nb_tab}
\begin{tabular}{ c c c } 
	    \noalign{\smallskip}\hline\noalign{\smallskip}
		Data & AUC \\
    	   \noalign{\smallskip}\hline\noalign{\smallskip}
		Train &  0.85311513\\
		Dev Set & 0.88468074\\
		\noalign{\smallskip}\hline\noalign{\smallskip}
  \end{tabular} 
\end{table}

\subsection{Neural Networks}
After trying different linear models, we did some experiments on non-linear models. We trained and tested the data set on Neural Networks. Instead of using a python package, we implemented the code for NN. We tuned the \textit{hidden layer} and \textit{learning rate} parameter,  and tried different loss functions - non-linear ones like :  \textit{Sigmoid Loss}, \textit{tanh Loss} \& linear loss function : \textit{Identity activation}. The data for results of NN is provided in Table \ref{tab_nn}.

\begin{table}[!htb]
 \centering
 \caption{AUC for Neural Networks}
 \label{tab_nn}
\begin{tabular}{ c c c } 
	    \noalign{\smallskip}\hline\noalign{\smallskip}
		Data &  AUC \\
    	   \noalign{\smallskip}\hline\noalign{\smallskip}
		Train &  0.8599\\
		Dev set &  0.8927\\
				\noalign{\smallskip}\hline\noalign{\smallskip}
  \end{tabular} 
\end{table}

Neural Networks did not perform as well as expected. This is because of the kind of dataset we have at hand. The dataset is linearly separable hence the traditional methods outperformed the more complex non-linear models. This shows the significance of dataset analysis prior to model selection. The most promising models may behave differently depending on the features of the dataset.  

\subsection{XGBoost Classifier}
In this model we used XGBoost classifier to train our model. We tried tuning various parameters \textit{subsample}, \textit{$\eta$}, \textit{colsample\_bytree}, \textit{max\_depth} and \textit{min\_child\_weight}. The best parameters were found to be \textit{$\eta$} : 0.001, \textit{subsample}: 0.3, \textit{colsample\_bytree}: 0.5, \textit{max\_depth}: 3, \textit{min\_child\_weight}: 9. The following table lists some results.  The number of boosting rounds used was 6000.
 
\begin{table}[!htb]
\centering
\caption{AUC for XGBoost}
\label{tab_xgb}
\begin{tabular}{ c c c }
                    \noalign{\smallskip}\hline\noalign{\smallskip}
                                Data  & AUC \\
                   \noalign{\smallskip}\hline\noalign{\smallskip}
                                Train & 0.86786\\
                                Dev set  & 0.88758\\  
                                		\noalign{\smallskip}\hline\noalign{\smallskip}	                   
  \end{tabular}
\end{table}
 
In Table \ref{tab_xgb} we represent the AUC for XGBoost, where the area under the ROC curve is 0.88758.
 
\begin{figure}
\centering
  \includegraphics[width=200px, height = 200px]{xgboost_dev}
  \caption{AUC for XGBoost}
  \label{xgboost_fig}
\end{figure}

\section{Conclusions}
Our experiments have shown that the models which rely on linear decision boundaries provide the best results on the given dataset. Among the transforms that we used, logarithimic transform proved to be the most effective because it removed the disparity between the various attributes. From the above experiments we can conclude that the AUC for all our models on the dev set varies between 0.87 - 0.89.
The following table provides a summary of AUC scores on Test Dataset for our submissions made on Kaggle website. We present the results of the best values of the models described above that we submitted. All the results presented here were the ones in which we used Feature selection, as well as did Z-Score \& Log normalization of the data.
\begin{table}[!htb]
\centering
\caption{AUC scores for Test Dataset (As per Kaggle Evaluation)}
\label{tab_lr}
\begin{tabular}{ c c c }
 \noalign{\smallskip}\hline\noalign{\smallskip}
   Model  & AUC \\
 \noalign{\smallskip}\hline\noalign{\smallskip}
 Logistic Regression & 0.86065\\
 XgBoost & 0.86168\\
 Gaussian Na{\"i}ve Bayes & 0.82009\\
 Neural Nets & 0.84182\\
 SVM & 0.83786 \\
 		\noalign{\smallskip}\hline\noalign{\smallskip}	
 \end{tabular}
\end{table}

From the table we see that XgBoost performs best since it is an emsemble of several weak classifiers. Followed by XgBoost is Logistic regression. This shows that the data was indeed linearly separable. We also learned about doing an analysis of the dataset, as through our analysis we figured out how the dataset was linearly separable and hence we were able to apply the traditional models instead of the complex models.
Future works on this problem could involve application of PCA to further select combinations of best attributes in order to drive better results.

% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}
\bibitem{kaggle}
A website for data science competitions. https://www.kaggle.com/
\bibitem{kaggle2}
Influencers in Social Networks : https://www.kaggle.com/c/predict-who-is-more-influential-in-a-social-network
\bibitem{roc}
https://en.wikipedia.org/wiki/Receiver\_operating\_characteristic
\bibitem{svm}
Sci-kit learn Support Vector Machines : http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html
\bibitem{gnb}
Sci-kit learn Gaussian Na{\"i}ve Bayes : http://scikit-learn.org/stable/modules/generated/sklearn.naive\_bayes.GaussianNB.html

\bibitem{xgboost}
XGBoost Library : https://pypi.python.org/pypi/xgboost/

\bibitem{ourrepo}
Our repo is hosted at : https://github.com/piyushghai/Social-Influencers.
\end{thebibliography}

% that's all folks
\end{document}
